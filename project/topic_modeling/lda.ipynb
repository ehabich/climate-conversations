{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jkyeaton/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "#stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent /home/jkyeaton/dsi_files_addtl/climate-conversations/project/projects/data/tokenized\n",
      "sys ['/home/jkyeaton/dsi_files_addtl/climate-conversations/project/topic_modeling', '/home/jkyeaton/miniconda3/envs/climate_env/lib/python311.zip', '/home/jkyeaton/miniconda3/envs/climate_env/lib/python3.11', '/home/jkyeaton/miniconda3/envs/climate_env/lib/python3.11/lib-dynload', '', '/home/jkyeaton/miniconda3/envs/climate_env/lib/python3.11/site-packages', '/home/jkyeaton/dsi_files_addtl/climate-conversations/project/projects/data/tokenized']\n"
     ]
    }
   ],
   "source": [
    "#parent_directory = os.path.abspath(os.path.join(os.getcwd(), '..', 'projects', 'data', 'tokenized'))\n",
    "#parent_directory = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "#print('parent', parent_directory)\n",
    "#sys.path.append(parent_directory)\n",
    "#print('sys', sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>gilded</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>tokenized_body_sents</th>\n",
       "      <th>tokenized_body_words</th>\n",
       "      <th>tokenized_body_words_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>iqkdvfz</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>Yea, exactly the media blame every storm on cl...</td>\n",
       "      <td>dragonfirebreather</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1664582500</td>\n",
       "      <td>iqkd247</td>\n",
       "      <td>xsg7fv</td>\n",
       "      <td>1664960560</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[yea, exactly the media blame every storm on c...</td>\n",
       "      <td>[yea, exactly, media, blame, storm, climate, c...</td>\n",
       "      <td>[yea, exactly, medium, blame, storm, climate, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>iqkebpl</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>We don’t deny climate change, we question the ...</td>\n",
       "      <td>the_stank__</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1664582726</td>\n",
       "      <td>xsg17k</td>\n",
       "      <td>xsg17k</td>\n",
       "      <td>1664960546</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[we don’t deny climate change, we question the...</td>\n",
       "      <td>[deny, climate, change, question, validity, fa...</td>\n",
       "      <td>[deny, climate, change, question, validity, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>iqkephh</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>It’s not just solar irradiance. It’s also the ...</td>\n",
       "      <td>johnnyg883</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1664582917</td>\n",
       "      <td>iqi8e8a</td>\n",
       "      <td>xrqls3</td>\n",
       "      <td>1664960535</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[it’s not just solar irradiance., it’s also th...</td>\n",
       "      <td>[solar, irradiance, solar, wind, cosmic, rays,...</td>\n",
       "      <td>[solar, irradiance, solar, wind, cosmic, ray, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>iqkf0a3</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>I’m with you. I’ll wait to see how cycle 25 pl...</td>\n",
       "      <td>johnnyg883</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1664583065</td>\n",
       "      <td>iqig4fr</td>\n",
       "      <td>xrqls3</td>\n",
       "      <td>1664960526</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[i’m with you., i’ll wait to see how cycle 25 ...</td>\n",
       "      <td>[wait, cycle, plays, threw, crystal, ball, def...</td>\n",
       "      <td>[wait, cycle, play, throw, crystal, ball, defe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>iqkfghm</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>That’s the most ridiculous comment I’ve seen s...</td>\n",
       "      <td>alexfromogish</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1664583293</td>\n",
       "      <td>iqivabo</td>\n",
       "      <td>xrentr</td>\n",
       "      <td>1664960511</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[that’s the most ridiculous comment i’ve seen ...</td>\n",
       "      <td>[ridiculous, comment, seen, started, watching,...</td>\n",
       "      <td>[ridiculous, comment, see, start, watch, andth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        subreddit  \\\n",
       "471   iqkdvfz  climateskeptics   \n",
       "713   iqkebpl  climateskeptics   \n",
       "924   iqkephh  climateskeptics   \n",
       "1082  iqkf0a3  climateskeptics   \n",
       "1323  iqkfghm  climateskeptics   \n",
       "\n",
       "                                                   body              author  \\\n",
       "471   Yea, exactly the media blame every storm on cl...  dragonfirebreather   \n",
       "713   We don’t deny climate change, we question the ...         the_stank__   \n",
       "924   It’s not just solar irradiance. It’s also the ...          johnnyg883   \n",
       "1082  I’m with you. I’ll wait to see how cycle 25 pl...          johnnyg883   \n",
       "1323  That’s the most ridiculous comment I’ve seen s...       alexfromogish   \n",
       "\n",
       "      score  gilded  created_utc parent_id link_id  retrieved_on  \\\n",
       "471       3       0   1664582500   iqkd247  xsg7fv    1664960560   \n",
       "713       9       0   1664582726    xsg17k  xsg17k    1664960546   \n",
       "924       1       0   1664582917   iqi8e8a  xrqls3    1664960535   \n",
       "1082      2       0   1664583065   iqig4fr  xrqls3    1664960526   \n",
       "1323      1       0   1664583293   iqivabo  xrentr    1664960511   \n",
       "\n",
       "      controversiality  is_submitter  \\\n",
       "471                  0          True   \n",
       "713                  0         False   \n",
       "924                  0         False   \n",
       "1082                 0         False   \n",
       "1323                 0         False   \n",
       "\n",
       "                                   tokenized_body_sents  \\\n",
       "471   [yea, exactly the media blame every storm on c...   \n",
       "713   [we don’t deny climate change, we question the...   \n",
       "924   [it’s not just solar irradiance., it’s also th...   \n",
       "1082  [i’m with you., i’ll wait to see how cycle 25 ...   \n",
       "1323  [that’s the most ridiculous comment i’ve seen ...   \n",
       "\n",
       "                                   tokenized_body_words  \\\n",
       "471   [yea, exactly, media, blame, storm, climate, c...   \n",
       "713   [deny, climate, change, question, validity, fa...   \n",
       "924   [solar, irradiance, solar, wind, cosmic, rays,...   \n",
       "1082  [wait, cycle, plays, threw, crystal, ball, def...   \n",
       "1323  [ridiculous, comment, seen, started, watching,...   \n",
       "\n",
       "                              tokenized_body_words_norm  \n",
       "471   [yea, exactly, medium, blame, storm, climate, ...  \n",
       "713   [deny, climate, change, question, validity, fa...  \n",
       "924   [solar, irradiance, solar, wind, cosmic, ray, ...  \n",
       "1082  [wait, cycle, play, throw, crystal, ball, defe...  \n",
       "1323  [ridiculous, comment, see, start, watch, andth...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your parent_directory definition\n",
    "parent_directory = os.path.abspath(os.path.join(os.getcwd(), '..', 'data', 'tokenized'))\n",
    "\n",
    "# Now construct the full path to the file\n",
    "file_path = os.path.join(parent_directory, 'tokenized_climateskeptics_sub_comments.pickle')\n",
    "\n",
    "# Try to open the file using the full path\n",
    "with open(file_path, 'rb') as f:\n",
    "    token_ck_sub_com = pickle.load(f)\n",
    "\n",
    "token_ck_sub_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in example dataset to help you understand what to do\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "#print(df.target_names.unique())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yea',\n",
       " 'exactly',\n",
       " 'medium',\n",
       " 'blame',\n",
       " 'storm',\n",
       " 'climate',\n",
       " 'change',\n",
       " 'florida',\n",
       " 'pirate',\n",
       " 'caribbean',\n",
       " 'territory',\n",
       " 'hurricane',\n",
       " 'normal',\n",
       " 'high',\n",
       " 'water',\n",
       " 'temperature',\n",
       " 'ask',\n",
       " 'jack',\n",
       " 'sparrow',\n",
       " 'amp',\n",
       " 'tell',\n",
       " 'normal']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# casting normalized words to a list to use going forward....?\n",
    "token_ck_sub_com_list = token_ck_sub_com.tokenized_body_words_norm.values.tolist()\n",
    "\n",
    "token_ck_sub_com_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(token_ck_sub_com_list, min_count=5, threshold=100) # JY: HAVE NOT YET THOUGHT ABOUT THESE NUMBERS\n",
    "trigram = gensim.models.Phrases(bigram[token_ck_sub_com_list], threshold=100)  # JY: HAVE NOT YET THOUGHT ABOUT THESE NUMBERS \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yea', 'exactly', 'medium', 'blame', 'storm', 'climate', 'change', 'florida', 'pirate', 'caribbean', 'territory', 'hurricane', 'normal', 'high', 'water', 'temperature', 'ask', 'jack', 'sparrow', 'amp', 'tell', 'normal']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[token_ck_sub_com_list[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(token_ck_sub_com_list)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exactly', 'medium', 'blame', 'storm', 'climate', 'change', 'territory', 'hurricane', 'normal', 'high', 'water', 'temperature', 'ask', 'tell', 'normal']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ask'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ask', 1),\n",
       "  ('blame', 1),\n",
       "  ('change', 1),\n",
       "  ('climate', 1),\n",
       "  ('exactly', 1),\n",
       "  ('high', 1),\n",
       "  ('hurricane', 1),\n",
       "  ('medium', 1),\n",
       "  ('normal', 2),\n",
       "  ('storm', 1),\n",
       "  ('tell', 1),\n",
       "  ('temperature', 1),\n",
       "  ('territory', 1),\n",
       "  ('water', 1)]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.093*\"uncertainty\" + 0.074*\"try\" + 0.057*\"problem\" + 0.056*\"scientific\" + '\n",
      "  '0.051*\"issue\" + 0.040*\"information\" + 0.039*\"great\" + 0.037*\"hot\" + '\n",
      "  '0.036*\"provide\" + 0.036*\"hurricane\"'),\n",
      " (1,\n",
      "  '0.246*\"reduce\" + 0.065*\"politician\" + 0.044*\"prevent\" + 0.041*\"destroy\" + '\n",
      "  '0.037*\"choice\" + 0.030*\"bill\" + 0.019*\"print\" + 0.017*\"offset\" + '\n",
      "  '0.000*\"heat\" + 0.000*\"radiation\"'),\n",
      " (2,\n",
      "  '0.207*\"know\" + 0.107*\"actually\" + 0.086*\"fact\" + 0.060*\"prediction\" + '\n",
      "  '0.051*\"word\" + 0.047*\"prove\" + 0.040*\"write\" + 0.031*\"well\" + 0.031*\"fail\" '\n",
      "  '+ 0.030*\"mention\"'),\n",
      " (3,\n",
      "  '0.168*\"say\" + 0.153*\"science\" + 0.077*\"different\" + 0.062*\"true\" + '\n",
      "  '0.058*\"start\" + 0.054*\"comment\" + 0.052*\"cold\" + 0.046*\"call\" + '\n",
      "  '0.042*\"little\" + 0.033*\"man\"'),\n",
      " (4,\n",
      "  '0.103*\"article\" + 0.073*\"read\" + 0.065*\"wrong\" + 0.064*\"day\" + '\n",
      "  '0.053*\"impact\" + 0.044*\"old\" + 0.038*\"account\" + 0.038*\"live\" + '\n",
      "  '0.033*\"free\" + 0.032*\"away\"'),\n",
      " (5,\n",
      "  '0.143*\"include\" + 0.086*\"area\" + 0.074*\"post\" + 0.069*\"refer\" + '\n",
      "  '0.066*\"appear\" + 0.064*\"interesting\" + 0.057*\"view\" + 0.038*\"field\" + '\n",
      "  '0.031*\"wait\" + 0.023*\"wiki\"'),\n",
      " (6,\n",
      "  '0.184*\"warm\" + 0.116*\"mean\" + 0.077*\"emit\" + 0.073*\"long\" + 0.072*\"air\" + '\n",
      "  '0.048*\"far\" + 0.041*\"consider\" + 0.035*\"case\" + 0.031*\"total\" + '\n",
      "  '0.030*\"support\"'),\n",
      " (7,\n",
      "  '0.148*\"big\" + 0.116*\"money\" + 0.093*\"government\" + 0.048*\"deny\" + '\n",
      "  '0.047*\"pay\" + 0.037*\"buy\" + 0.027*\"spend\" + 0.024*\"expensive\" + 0.024*\"tax\" '\n",
      "  '+ 0.023*\"job\"'),\n",
      " (8,\n",
      "  '0.149*\"year\" + 0.066*\"high\" + 0.058*\"level\" + 0.052*\"go\" + 0.047*\"average\" '\n",
      "  '+ 0.045*\"rise\" + 0.044*\"trend\" + 0.025*\"c\" + 0.025*\"see\" + '\n",
      "  '0.025*\"difference\"'),\n",
      " (9,\n",
      "  '0.147*\"temperature\" + 0.108*\"co2\" + 0.074*\"gas\" + 0.071*\"increase\" + '\n",
      "  '0.067*\"atmosphere\" + 0.062*\"model\" + 0.057*\"surface\" + 0.055*\"effect\" + '\n",
      "  '0.046*\"earth\" + 0.028*\"atmospheric\"'),\n",
      " (10,\n",
      "  '0.112*\"look\" + 0.081*\"study\" + 0.077*\"talk\" + 0.051*\"theory\" + '\n",
      "  '0.041*\"solar\" + 0.040*\"conclusion\" + 0.037*\"sure\" + 0.034*\"bit\" + '\n",
      "  '0.031*\"important\" + 0.031*\"pretty\"'),\n",
      " (11,\n",
      "  '0.133*\"world\" + 0.124*\"bad\" + 0.109*\"real\" + 0.076*\"country\" + '\n",
      "  '0.063*\"renewable\" + 0.048*\"definition\" + 0.030*\"attention\" + '\n",
      "  '0.030*\"literally\" + 0.029*\"oil\" + 0.025*\"dependent\"'),\n",
      " (12,\n",
      "  '0.085*\"predict\" + 0.069*\"course\" + 0.066*\"process\" + 0.053*\"lie\" + '\n",
      "  '0.049*\"lol\" + 0.041*\"decrease\" + 0.040*\"natural\" + 0.039*\"history\" + '\n",
      "  '0.036*\"sort\" + 0.030*\"ability\"'),\n",
      " (13,\n",
      "  '0.272*\"climate\" + 0.197*\"change\" + 0.081*\"global\" + 0.058*\"cause\" + '\n",
      "  '0.047*\"warming\" + 0.031*\"weather\" + 0.029*\"human\" + 0.029*\"record\" + '\n",
      "  '0.027*\"period\" + 0.017*\"planet\"'),\n",
      " (14,\n",
      "  '0.087*\"think\" + 0.062*\"thing\" + 0.061*\"point\" + 0.057*\"way\" + 0.041*\"want\" '\n",
      "  '+ 0.041*\"find\" + 0.040*\"claim\" + 0.039*\"work\" + 0.037*\"show\" + 0.035*\"get\"'),\n",
      " (15,\n",
      "  '0.101*\"energy\" + 0.076*\"time\" + 0.043*\"happen\" + 0.040*\"low\" + 0.039*\"need\" '\n",
      "  '+ 0.035*\"water\" + 0.032*\"come\" + 0.031*\"understand\" + 0.029*\"small\" + '\n",
      "  '0.028*\"system\"'),\n",
      " (16,\n",
      "  '0.112*\"rate\" + 0.090*\"paper\" + 0.085*\"source\" + 0.068*\"link\" + '\n",
      "  '0.048*\"describe\" + 0.046*\"continue\" + 0.044*\"hear\" + 0.044*\"exactly\" + '\n",
      "  '0.028*\"hypothesis\" + 0.027*\"control\"'),\n",
      " (17,\n",
      "  '0.181*\"nonsense\" + 0.095*\"check\" + 0.084*\"determine\" + 0.057*\"disagree\" + '\n",
      "  '0.050*\"formula\" + 0.017*\"surround\" + 0.012*\"quantify\" + 0.000*\"heat\" + '\n",
      "  '0.000*\"radiation\" + 0.000*\"object\"'),\n",
      " (18,\n",
      "  '0.223*\"good\" + 0.085*\"thank\" + 0.072*\"poor\" + 0.066*\"run\" + 0.062*\"step\" + '\n",
      "  '0.044*\"carbon\" + 0.040*\"admit\" + 0.033*\"available\" + 0.022*\"reasoning\" + '\n",
      "  '0.021*\"fair\"'),\n",
      " (19,\n",
      "  '0.171*\"people\" + 0.079*\"scientist\" + 0.069*\"make\" + 0.065*\"tell\" + '\n",
      "  '0.045*\"believe\" + 0.045*\"end\" + 0.031*\"turn\" + 0.029*\"life\" + '\n",
      "  '0.028*\"suggest\" + 0.028*\"significant\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -17.05596264208413\n",
      "\n",
      "Coherence Score:  0.3423718862169061\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyLDAvis' has no attribute 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize the topics\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[0;32m----> 3\u001b[0m vis \u001b[38;5;241m=\u001b[39m pyLDAvis\u001b[38;5;241m.\u001b[39mgensim\u001b[38;5;241m.\u001b[39mprepare(lda_model, corpus, id2word)\n\u001b[1;32m      4\u001b[0m vis\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyLDAvis' has no attribute 'gensim'"
     ]
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
