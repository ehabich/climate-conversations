{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzfudzo_65h2"
      },
      "source": [
        "This notebook is used to run the tokenizer and and moral classifier in parallel on Google CoLab's High Ram CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX3h9mElehhq",
        "outputId": "4fa2a6c7-9d28-4b34-ae65-92f56455fb44"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40jiv-DKgvUU",
        "outputId": "09743690-52cb-41f0-b6ad-02b4c7a10c0c"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/climate-conversations-kathryn/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxauRs32ejKz",
        "outputId": "ffa9ddb4-d216-4cc9-fa47-30e470b10133"
      },
      "outputs": [],
      "source": [
        "!apt-get install python3.11\n",
        "!ln -sf /usr/bin/python3.11 /usr/local/bin/python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWZFlk3yeq1K"
      },
      "outputs": [],
      "source": [
        "# !pip install poetry\n",
        "# !poetry config virtualenvs.in-project true\n",
        "# !poetry install --no-ansi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZCOs9E9lOvv"
      },
      "outputs": [],
      "source": [
        "!source .venv/bin/activate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_2dr_kkesdz",
        "outputId": "4e681312-63aa-4471-bd73-3f566c02e2e8"
      },
      "outputs": [],
      "source": [
        "%cd project/analysis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "ydb7iYuaBZv9",
        "outputId": "5b747611-3d9c-41e2-bfec-fe72e1876170"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def run_subreddit(subreddit):\n",
        "    \"\"\"Function to run the moral_strength_classifier.py script with specific subreddit and other parameters.\"\"\"\n",
        "    # Define the base command and parameters\n",
        "    base_command = \"python3 moral_strength_classifier.py\"\n",
        "    input_data_file_path = \"project/data_collection/project_data/tokenized_all_submissions.pickle\"\n",
        "    col_to_tokenize = 'title'\n",
        "    type = 'submission'\n",
        "    # Assuming 'tokenize' is a boolean flag that should be passed as such\n",
        "    tokenize = 'False'\n",
        "\n",
        "    # Format the command with all parameters\n",
        "    formatted_command = f\"{base_command} --filepath {input_data_file_path} --col_to_tokenize {col_to_tokenize} --subreddit {subreddit} --tokenize {tokenize} --type {type}\"\n",
        "\n",
        "    print(f\"Running command: {formatted_command}\")\n",
        "    try:\n",
        "        output = subprocess.check_output(formatted_command, stderr=subprocess.STDOUT, shell=True, universal_newlines=True)\n",
        "        print(f\"Command output:\\n{output}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error executing command:\\n{e.output}\")\n",
        "\n",
        "def main():\n",
        "    subreddits = [\n",
        "      'climateskeptics',\n",
        "      'climate',\n",
        "      'environment',\n",
        "      'climatechange',\n",
        "      'climateoffensive',\n",
        "      'science',\n",
        "      'globalnews',\n",
        "      'politics'\n",
        "    ]\n",
        "\n",
        "    # Run the script sequentially for each subreddit\n",
        "    for subreddit in subreddits:\n",
        "        run_subreddit(subreddit)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "dYpWbZ7E0taF",
        "outputId": "7cc90508-03dc-486c-8394-ed233e664270"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import multiprocessing\n",
        "\n",
        "def run_subreddit(subreddit):\n",
        "    \"\"\"Function to run the moral_strength_classifier.py script with specific subreddit and other parameters.\"\"\"\n",
        "    # Define the base command and parameters\n",
        "    base_command = \"python3 moral_strength_classifier.py\"\n",
        "    input_data_file_path = \"project/data_collection/project_data/tokenized_climate_comments.pickle\"\n",
        "    col_to_tokenize = 'Body'\n",
        "    token_name = 'tokenized_body'\n",
        "    # Assuming 'tokenize' is a boolean flag that should be passed as such\n",
        "    tokenize = 'False'\n",
        "\n",
        "    # Format the command with all parameters\n",
        "    formatted_command = f\"{base_command} --filepath {input_data_file_path} --col_to_tokenize {col_to_tokenize} --token_name {token_name} --subreddit {subreddit} --tokenize {tokenize}\"\n",
        "\n",
        "    print(f\"Running command: {formatted_command}\")\n",
        "    try:\n",
        "        output = subprocess.check_output(formatted_command, stderr=subprocess.STDOUT, shell=True, universal_newlines=True)\n",
        "        print(f\"Command output:\\n{output}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error executing command:\\n{e.output}\")\n",
        "\n",
        "def main():\n",
        "    subreddits = [\n",
        "      'climateskeptics',\n",
        "      'climate',\n",
        "      #'environment',\n",
        "      #'climatechange',\n",
        "      #'climateoffensive',\n",
        "      #'science',\n",
        "      #'globalnews',\n",
        "      #'politics',\n",
        "    ]\n",
        "\n",
        "    # Use multiprocessing to run the script for each subreddit\n",
        "    with multiprocessing.Pool(processes=min(len(subreddits), multiprocessing.cpu_count())) as pool:\n",
        "        pool.map(run_subreddit, subreddits)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
